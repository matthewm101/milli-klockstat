{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milli-kLockStat Analysis\n",
    "## To start, run the following cell, replacing the filename and sample period (if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# The sample period in ns. Will typically be 50000000 (50ms).\n",
    "sample_period_ns = 50000000\n",
    "sample_period_ms = sample_period_ns // 1000000\n",
    "\n",
    "# The filename to read data from.\n",
    "filename = \"LOG_NAME.log\"\n",
    "# Add any commands that you want filtered here.\n",
    "filtered_commands = [\"collect\", \"collectl\", \"tcplife_bpfcc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Collection\n",
    "## Run the following cell to read the file and collect statistics for all the remaining cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, comm, ptids, locktype, lockaddr, ts, totalwait, count, traceid):\n",
    "        self.comm = comm.strip()\n",
    "        self.ptids = ptids.strip()\n",
    "        self.locktype = locktype.strip()\n",
    "        self.lockaddr = lockaddr.strip()\n",
    "        self.ts = int(ts)\n",
    "        self.totalwait = int(totalwait)\n",
    "        self.count = int(count)\n",
    "        self.avgwait = self.totalwait / self.count\n",
    "        self.traceid = int(traceid)\n",
    "\n",
    "earliest_timestamp = [0x0FFF_FFFF_FFFF_FFFF]\n",
    "latest_timestamp = [0]\n",
    "samples = [] # Every single sample.\n",
    "samples_by_timestamp = {} # Key: a timestamp. Value: a list of indices into the sample array.\n",
    "samples_by_addr = {} # Key: a lock address. Value: a list of indices into the sample array.\n",
    "total_access_counts_per_addr = {} # Key: a lock address. Value: the total number of accesses for that lock across all samples.\n",
    "accumulated_access_time_per_addr = {} # Key: a lock address. Value: the sum of all access times for that lock across all samples.\n",
    "worst_access_times_per_addr = {} # Key: a lock address. Value: the worst average access time for a period across all samples.\n",
    "total_access_counts_per_addr_then_timestamp = {} # First key: the lock address. Second key: the timestamp. Value: the sum of all access counts across all threads for one sample.\n",
    "accumulated_access_time_per_addr_then_timestamp = {} # First key: the lock address. Second key: the timestamp. Value: the sum of all access times across all threads for one sample.\n",
    "contenders_by_addr = {} # Key: the lock address. Value: the set of process/thread IDs that accessed that lock over the entire log.\n",
    "contenders_by_addr_then_timestamp = {} # First key: the lock address. Second key: the timestamp. Value: the set of process/thread IDs that accessed that lock over the entire log.\n",
    "worst_access_times_by_timestamp = {} # Key: a timestamp of a period. Value: the worst AAT experienced by any lock for that period.\n",
    "first_samples_by_lock_then_ptid = {} # First key: a lock address. Second key: a process+thread ID. Value: a sample index.\n",
    "\n",
    "traces = {}\n",
    "\n",
    "def safe_append(d, k, v):\n",
    "    if not k in d.keys():\n",
    "        d[k] = [v]\n",
    "    else:\n",
    "        d[k].append(v)\n",
    "\n",
    "def safe_setadd(d, k, v):\n",
    "    if not k in d.keys():\n",
    "        d[k] = set([v])\n",
    "    else:\n",
    "        d[k].add(v)\n",
    "\n",
    "def safe_add(d, k, v):\n",
    "    if not k in d.keys():\n",
    "        d[k] = v\n",
    "    else:\n",
    "        d[k] += v\n",
    "\n",
    "def double_safe_add(d, k1, k2, v):\n",
    "    if not k1 in d.keys():\n",
    "        d[k1] = {}\n",
    "    if not k2 in d[k1].keys():\n",
    "        d[k1][k2] = v\n",
    "    else:\n",
    "        d[k1][k2] += v\n",
    "\n",
    "def double_safe_setadd(d, k1, k2, v):\n",
    "    if not k1 in d.keys():\n",
    "        d[k1] = {}\n",
    "    if not k2 in d[k1].keys():\n",
    "        d[k1][k2] = set([v])\n",
    "    else:\n",
    "        d[k1][k2].add(v)\n",
    "\n",
    "def safe_max(d, k, v):\n",
    "    if not k in d.keys():\n",
    "        d[k] = v\n",
    "    else:\n",
    "        d[k] = max(d[k], v)\n",
    "\n",
    "def safe_get(d, k1, default):\n",
    "    if k1 not in d.keys():\n",
    "        return default\n",
    "    else:\n",
    "        return d[k1]\n",
    "\n",
    "def safe_double_get(d, k1, k2, default):\n",
    "    if k1 not in d.keys():\n",
    "        return default\n",
    "    elif k2 not in d[k1].keys():\n",
    "        return default\n",
    "    else:\n",
    "        return d[k1][k2]\n",
    "\n",
    "def safe_double_emplace(d, k1, k2, v):\n",
    "    if not k1 in d.keys():\n",
    "        d[k1] = {}\n",
    "    if not k2 in d[k1].keys():\n",
    "        d[k1][k2] = v\n",
    "\n",
    "def add_sample(line):\n",
    "    sample = Sample(line[0], line[1], line[2], line[3], line[4], line[5], line[6], line[7])\n",
    "    if not (sample.comm in filtered_commands):\n",
    "        latest_timestamp[0] = max(latest_timestamp[0], sample.ts)\n",
    "        earliest_timestamp[0] = min(earliest_timestamp[0], sample.ts)\n",
    "        samples.append(sample)\n",
    "        i = len(samples) - 1\n",
    "        safe_append(samples_by_timestamp, sample.ts, i)\n",
    "        safe_append(samples_by_addr, sample.lockaddr, i)\n",
    "        safe_add(total_access_counts_per_addr, sample.lockaddr, sample.count)\n",
    "        safe_add(accumulated_access_time_per_addr, sample.lockaddr, sample.totalwait)\n",
    "        safe_max(worst_access_times_per_addr, sample.lockaddr, sample.avgwait)\n",
    "        double_safe_add(total_access_counts_per_addr_then_timestamp, sample.lockaddr, sample.ts, sample.count)\n",
    "        double_safe_add(accumulated_access_time_per_addr_then_timestamp, sample.lockaddr, sample.ts, sample.totalwait)\n",
    "        safe_setadd(contenders_by_addr, sample.lockaddr, sample.ptids)\n",
    "        double_safe_setadd(contenders_by_addr_then_timestamp, sample.lockaddr, sample.ts, sample.ptids)\n",
    "        safe_max(worst_access_times_by_timestamp, sample.ts, sample.avgwait)\n",
    "        safe_double_emplace(first_samples_by_lock_then_ptid, sample.lockaddr, sample.ptids, i)\n",
    "\n",
    "file = open(filename)\n",
    "lines = csv.reader(file)\n",
    "for line in lines:\n",
    "    if len(line) > 0:\n",
    "        if line[0] == \"s\":\n",
    "            add_sample(line[1:])\n",
    "        elif line[0] == \"t\":\n",
    "            traces[int(line[1])] = line[2:]\n",
    "file.close()\n",
    "\n",
    "# Modify these if you wish to use custom timestamps for graphing\n",
    "latest_timestamp = latest_timestamp[0]\n",
    "earliest_timestamp = earliest_timestamp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Most Contended Locks\n",
    "## Run this cell to list the locks that have been accessed by the most threads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_contended_locks = []\n",
    "for addr in samples_by_addr.keys():\n",
    "    most_contended_locks.append((len(contenders_by_addr[addr]), addr))\n",
    "most_contented_locks = sorted(most_contended_locks, reverse=True)\n",
    "print(\"20 most contended locks (sorted by highest number of unique threads that accessed each lock)\")\n",
    "for aat, addr in most_contented_locks[:20]:\n",
    "    print(f\"{addr}\\t{aat} threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Contention For Individual Addresses\n",
    "## Run this cell to graph the number of threads simultaneously accessing a lock over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = \"0xLOCKADDRESSHERE\"\n",
    "\n",
    "# Modify these values to shrink the time span shown in the graph\n",
    "# graph_earliest_timestamp = earliest_timestamp\n",
    "graph_earliest_timestamp = latest_timestamp - 720 * 1000000000\n",
    "graph_latest_timestamp = latest_timestamp\n",
    "\n",
    "graph_earliest_timestamp = (graph_earliest_timestamp // sample_period_ns) * sample_period_ns\n",
    "graph_latest_timestamp = (graph_latest_timestamp // sample_period_ns) * sample_period_ns\n",
    "valid_timestamps = np.arange(graph_earliest_timestamp, graph_latest_timestamp + sample_period_ns, sample_period_ns)\n",
    "aats = np.array([len(safe_get(contenders_by_addr_then_timestamp[lock], i, set([]))) for i in valid_timestamps]) / 1000000 # convert to ms\n",
    "easy_to_read_timestamps = (valid_timestamps - graph_earliest_timestamp) // 1000000000 # convert to s\n",
    "plt.rcParams[\"figure.figsize\"] = (30,5)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Worst Wait Time (ms)\")\n",
    "plt.plot(easy_to_read_timestamps, aats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Threads Accessing a Lock\n",
    "## Run this cell to list the threads, processes, and commands that are using a specific lock, along with stack traces for each thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lock = \"0xLOCKADDRESSHERE\"\n",
    "first_samples_by_ptid = first_samples_by_lock_then_ptid[lock]\n",
    "for sampleid in first_samples_by_ptid.values():\n",
    "    sample = samples[sampleid]\n",
    "    print(f\"Comm: {sample.comm}, PTIDS: {sample.ptids}, Trace: {traces[sample.traceid]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Worst Locks by AAT\n",
    "## Run this cell to list the locks that experienced the worse average access time over the entire thread.\n",
    "Note: this is calculated by summing the total wait time across all accesses for the entire log, divided by the total number of accesses for the entire log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this number to be larger than 1 if you want to filter out locks that were only used by 1 thread.\n",
    "min_required_threads_using = 1\n",
    "\n",
    "duration = (latest_timestamp - earliest_timestamp) / 1e9\n",
    "print(f\"Timestamp span: {earliest_timestamp}-{latest_timestamp} ({duration} seconds)\")\n",
    "\n",
    "worst_total_aats = []\n",
    "for addr in samples_by_addr.keys():\n",
    "    if len(contenders_by_addr[addr]) >= min_required_threads_using:\n",
    "        worst_total_aats.append((accumulated_access_time_per_addr[addr] / total_access_counts_per_addr[addr], addr))\n",
    "worst_total_aats = sorted(worst_total_aats, reverse=True)\n",
    "print(\"20 worst locks by AAT, averaged over the entire sampling time\")\n",
    "for aat, addr in worst_total_aats[:20]:\n",
    "    print(f\"{addr}\\t{aat} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Worst Locks by Worst Period AAT\n",
    "## Run this cell to list the locks that experienced the worst average latency for some 50ms period during the log.\n",
    "This is useful for detecting the worst spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this number to be larger than 1 if you want to filter out locks that were only used by 1 thread.\n",
    "min_required_threads_using = 1\n",
    "\n",
    "locks_to_print = 10\n",
    "\n",
    "worst_single_sample_aats = []\n",
    "for addr in samples_by_addr.keys():\n",
    "    if len(contenders_by_addr[addr]) >= min_required_threads_using:\n",
    "        worst_single_sample_aats.append((worst_access_times_per_addr[addr], addr))\n",
    "worst_single_sample_aats = sorted(worst_single_sample_aats, reverse=True)\n",
    "print(f\"{locks_to_print} worst locks, based on the worst AAT observed by a thread using the lock over some {sample_period_ms}ms period\")\n",
    "for aat, addr in worst_single_sample_aats[:locks_to_print]:\n",
    "    print(f\"{addr}\\t{aat} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Worst Waits for Any Lock\n",
    "## Run this cell to graph the worst average wait time for ANY lock for all sample periods.\n",
    "This is useful to locating particularly bad contention spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify these values to shrink the time span shown in the graph\n",
    "# graph_earliest_timestamp = earliest_timestamp\n",
    "graph_earliest_timestamp = latest_timestamp - 720 * 1000000000\n",
    "graph_latest_timestamp = latest_timestamp\n",
    "\n",
    "graph_earliest_timestamp = (graph_earliest_timestamp // sample_period_ns) * sample_period_ns\n",
    "graph_latest_timestamp = (graph_latest_timestamp // sample_period_ns) * sample_period_ns\n",
    "valid_timestamps = np.arange(graph_earliest_timestamp, graph_latest_timestamp + sample_period_ns, sample_period_ns)\n",
    "aats = np.array([safe_get(worst_access_times_by_timestamp, i, 0) for i in valid_timestamps]) / 1000000 # convert to ms\n",
    "easy_to_read_timestamps = (valid_timestamps - graph_earliest_timestamp) // 1000000000 # convert to s\n",
    "plt.rcParams[\"figure.figsize\"] = (30,5)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Worst Wait Time (ms)\")\n",
    "plt.plot(easy_to_read_timestamps, aats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Lock Graphing\n",
    "## Run this cell to graph the average lock wait time over the course of the entire log, for a specific lock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this to check a different lock\n",
    "lock_to_graph = \"0xLOCKADDRESSHERE\"\n",
    "\n",
    "# Modify these values to shrink the time span shown in the graph\n",
    "# graph_earliest_timestamp = earliest_timestamp\n",
    "graph_earliest_timestamp = latest_timestamp - 720 * 1000000000\n",
    "graph_latest_timestamp = latest_timestamp\n",
    "\n",
    "graph_earliest_timestamp = (graph_earliest_timestamp // sample_period_ns) * sample_period_ns\n",
    "graph_latest_timestamp = (graph_latest_timestamp // sample_period_ns) * sample_period_ns\n",
    "valid_timestamps = np.arange(graph_earliest_timestamp, graph_latest_timestamp + sample_period_ns, sample_period_ns)\n",
    "acc_acc_times = accumulated_access_time_per_addr_then_timestamp[lock_to_graph]\n",
    "tot_acc_counts = total_access_counts_per_addr_then_timestamp[lock_to_graph]\n",
    "aats = [safe_get(acc_acc_times, i, 0) / safe_get(tot_acc_counts, i, 1) for i in valid_timestamps]\n",
    "easy_to_read_timestamps = (valid_timestamps - graph_earliest_timestamp) // 1000000000\n",
    "plt.rcParams[\"figure.figsize\"] = (30,5)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Worst Wait Time (ms)\")\n",
    "plt.plot(easy_to_read_timestamps, aats)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
